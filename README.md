# ğŸ‘ï¸ Blind Assistance

## ğŸ“Œ Overview

Blind Assistance is a real-time object detection and text-to-speech system designed to assist visually impaired individuals. The project uses **YOLOv8** for object detection and **pyttsx3** for speech synthesis to announce detected objects with their directional position. ğŸ¤

## âœ¨ Features

- ğŸ” Real-time object detection using YOLOv8 (nano model for performance).
- ğŸ—£ï¸ Converts detected objects into speech with directional guidance (left, center, right).
- ğŸ“· Uses OpenCV for camera integration and visualization.
- ğŸ¯ Supports multiple object detection with confidence scores.
- ğŸ• Intelligent speech throttling to avoid repetitive announcements.
- ğŸ›ï¸ Optimized performance with frame skipping and reduced resolution.
- ğŸ§µ Multi-threaded text-to-speech for smooth operation.

## âš™ï¸ Requirements

Ensure you have the following dependencies installed:

```bash
pip install -r requirements.txt
```

Or install manually:

```bash
pip install opencv-python numpy pyttsx3 ultralytics torch torchvision Pillow
```

The YOLOv8 nano model (`yolov8n.pt`) will be automatically downloaded when you first run the script.

## ğŸš€ Setup

1. Clone the repository:
   ```bash
   git clone https://github.com/vikranth2711/blind-assistance.git
   cd blind-assistance
   ```
2. Install dependencies:
   ```bash
   pip install -r requirements.txt
   ```
3. Run the script:
   ```bash
   python project-final.py
   ```

## ğŸ¥ How It Works

1. ğŸ“¸ The camera captures real-time video frames at 640x480 resolution.
2. ğŸ§  YOLOv8 detects objects with a minimum confidence threshold of 0.5.
3. ğŸ“ The system determines object position (left, center, right) based on bounding box center.
4. ğŸ–¼ï¸ Bounding boxes and labels with directional info are drawn on detected objects.
5. ğŸ”Š Objects are announced with their direction (e.g., "person on the left").
6. â±ï¸ Speech throttling prevents repeated announcements of the same object within 5 seconds.
7. ğŸ”„ The system processes every 2nd frame for optimal performance.
8. âŒ Press 'q' to exit the application.

## ğŸ›ï¸ Configuration Options

You can modify these constants in the code:

- `FRAME_SKIP = 2`: Process every nth frame (higher = better performance, lower accuracy)
- `FRAME_WIDTH, FRAME_HEIGHT = 640, 480`: Camera resolution
- `CONFIDENCE_THRESHOLD = 0.5`: Minimum detection confidence (0.0-1.0)
- `SPEECH_INTERVAL = 5`: Seconds between same object announcements
- Voice settings: Rate (150 WPM) and volume (0.9)

## ğŸ”§ Technical Details

- **Model**: YOLOv8n (nano) - optimized for real-time inference
- **Framework**: Ultralytics YOLO
- **Dataset**: Pre-trained on COCO dataset (80+ object classes)
- **Performance**: ~30 FPS on modern hardware with frame skipping
- **Memory**: Low memory footprint with nano model

## ğŸ“‹ Supported Objects

The system can detect 80+ object classes from the COCO dataset including:

- People, vehicles, animals
- Furniture, electronics, kitchen items
- Sports equipment, tools, and more

## ğŸ”® Future Improvements

- âš¡ Support for YOLOv8s/m/l models for better accuracy.
- ğŸ“ Add distance estimation using depth cameras or stereo vision.
- ğŸ“± Mobile app version with camera integration.
- ğŸ—£ï¸ Enhanced speech with object descriptions and contextual information.
- ğŸ¯ Custom training for specific environments or objects.
- ğŸ”Š Audio cues and spatial audio for better directional guidance.
- âš™ï¸ GUI configuration panel for real-time parameter adjustment.

## ğŸ¤ Contributing

Contributions are welcome! Feel free to open issues or submit pull requests. ğŸ’¡

### Development Setup

1. Fork the repository
2. Create a feature branch: `git checkout -b feature-name`
3. Make your changes and test thoroughly
4. Submit a pull request with detailed description

## ğŸ“„ License

This project is open source. Please check the repository for license details.

## ğŸ™ Acknowledgments

- [Ultralytics](https://ultralytics.com/) for YOLOv8
- [OpenCV](https://opencv.org/) for computer vision tools
- [pyttsx3](https://pypi.org/project/pyttsx3/) for text-to-speech functionality
